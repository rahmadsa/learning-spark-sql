{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**A.Manipulasi Data Dengan Apache Spark**\n",
        "\n",
        "##**1. Getting Started with PySpark in Google Colab**\n",
        "\n",
        "*PySpark* adalah *interface Python* untuk *Apache Spark*. Penggunaan utama *PySpark* adalah untuk bekerja dengan data dalam Bigdata dan untuk membuat pipeline data.\n",
        "\n",
        "Walaupun *Apache Spark* mendudukung Big Data, sebagai awal pembelajaran tidak perlu menggunakan data yang besar untuk mendapatkan manfaat dari PySpark. Kita bisa temukan bahwa SparkSQL adalah tools yang bagus untuk melakukan analisis data. Penggunaan library Panda menjadi lambat dengan data yang besar\n",
        "\n",
        "Sumber tentang Apache Spark http://spark.apache.org/docs/latest/api/python/\n"
      ],
      "metadata": {
        "id": "Gun9kIlx5fJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Init Hadoop & Spark\n",
        "#=====Begin\n",
        "\n",
        "#=====END"
      ],
      "metadata": {
        "id": "YuG-AU1b5gUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load Data Opsi 1 (down load data to local)**"
      ],
      "metadata": {
        "id": "1R-wQYrfErmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2.1 **Reading Data**"
      ],
      "metadata": {
        "id": "tV58lZSyGNpw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzvNxiQSixRU"
      },
      "source": [
        "import requests\n",
        "path = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "req = requests.get(path)\n",
        "url_content = req.content\n",
        "\n",
        "csv_file_name = 'owid-covid-data.csv'\n",
        "csv_file = open(csv_file_name, 'wb')\n",
        "\n",
        "csv_file.write(url_content)\n",
        "csv_file.close()\n",
        "\n",
        "#fill data frame\n",
        "df_1 = spark.read.csv('/content/'+csv_file_name, header=True, inferSchema=True)\n",
        "\n",
        "#Viewing the dataframe schema\n",
        "df_1.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRUC46L_8zX"
      },
      "source": [
        "## 2.2. **PySpark DataFrames**\n",
        "\n",
        "**DataFrame** adalah struktur data dua dimensi dalam bahasa pemrograman komputer, mirip dengan tabel pada microsoft Excel. Pada pemrograman Python strukur data DataFrame adalah objek dalam pustaka panda. Sepeti halnya Panda Apache Spark juga memliki struktur data tersebut untuk pemrosesan data yang lebih besar yaitu **BigData**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulasi data dengan SQL**\n",
        "\n",
        "Spark SQL adalah modul dalam Apache Spark yang memungkinkan pemrosesan data terstruktur dan semi-terstruktur menggunakan SQL. Ini memungkinkan pengguna untuk menjalankan kueri SQL pada data yang disimpan dalam bentuk DataFrames di Spark, seperti yang diimplementasikan dalam framework DataFrames yang ada di Spark."
      ],
      "metadata": {
        "id": "Ga-As3PqHULg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFcoi5l7kyLq"
      },
      "source": [
        "#Manipulasi dataframe dengan SQL\n",
        "df_1 = spark.sql(\"SELECT * from covid_data\")\n",
        "\n",
        "df_1.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teHD2Up4k4Cd"
      },
      "source": [
        "groupDF = spark.sql(\"SELECT location, count(*) from covid_data group by location\")\n",
        "\n",
        "groupDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## **3. Load Data Opsi 2 (Using Panda)**"
      ],
      "metadata": {
        "id": "PYIwfW0FIE8m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-To1oW2S4mZL"
      },
      "source": [
        "##Method 2 use Using Panda Data Frme\n",
        "import  pandas as pd\n",
        "\n",
        "#URL remoting dataset\n",
        "url_data = \"https://github.com/rahmadsa/dataset/blob/main/sample_csv.csv?raw=true\"\n",
        "\n",
        "#fill data into panda dataframe from URL remoting dataset\n",
        "pd_df = pd.read_csv(url_data)\n",
        "\n",
        "#fill data into spark data frame from panda dataframe\n",
        "df_spark = spark.createDataFrame(pd_df)\n",
        "\n",
        "#create temporary view table\n",
        "df_spark.createOrReplaceTempView(\"sample_data\") #temporary view\n",
        "\n",
        "#test use query using SELECT\n",
        "df_2 = spark.sql(\"SELECT * from sample_data\")\n",
        "\n",
        "#test use show schema the view table\n",
        "df_2.printSchema()\n",
        "\n",
        "#test use show 5 rows the view table\n",
        "df_2.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Load Data Opsi 3 (Direct load using spark DataFrame)**"
      ],
      "metadata": {
        "id": "gfdTvtcSJD7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 3 use Get HTTP\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "url_github = 'https://github.com/rahmadsa/dataset/blob/main/sample_csv.csv?raw=true'\n",
        "\n",
        "#download CSV from URL\n",
        "spark.sparkContext.addFile(url_github)\n",
        "\n",
        "#read downloaded CSV\n",
        "df_3 = spark.read.csv(SparkFiles.get(\"sample_csv.csv\"),inferSchema=True, header=True)\n",
        "\n",
        "#create temporary view table\n",
        "df_3.createOrReplaceTempView(\"data_penjualan\") #temporary view\n",
        "\n",
        "#test use query using SELECT\n",
        "df_3 = spark.sql(\"SELECT * from data_penjualan\")\n",
        "\n",
        "#test use show schema the view table\n",
        "df_3.printSchema()\n",
        "\n",
        "#test use show 5 rows the view table\n",
        "df_3.show(5)\n"
      ],
      "metadata": {
        "id": "yD2X59_IdQB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tugas**\n",
        "\n",
        "Lakukan analisa dari dataset : https://github.com/rahmadsa/dataset/blob/main/sample_csv.csv'. Untuk mencari informasi penjualan sebagai berikut :\n",
        "\n",
        "1. Jumlah data transaksi dari dataset diatas\n",
        "2. Brand yang paling laku\n",
        "3. Harga Brand yang paling laku\n",
        "4. Brand dengan penjualan paling sedikit\n"
      ],
      "metadata": {
        "id": "-qR02dgo2gaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#jawab\n"
      ],
      "metadata": {
        "id": "B592aAqt4HVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
